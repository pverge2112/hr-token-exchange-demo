services:

  kong-gateway:
    image: kong/kong-gateway:3.13
    ports:
      - "8000:8000"
      - "8443:8443"
    extra_hosts:
      - "localhost:host-gateway"
    environment:
      KONG_ROLE: data_plane
      KONG_DATABASE: "off"
      KONG_VITALS: "off"
      KONG_CLUSTER_MTLS: pki
      KONG_CLUSTER_CONTROL_PLANE: ${KONG_CLUSTER_CONTROL_PLANE}
      KONG_CLUSTER_SERVER_NAME: ${KONG_CLUSTER_SERVER_NAME}
      KONG_CLUSTER_TELEMETRY_ENDPOINT: ${KONG_CLUSTER_TELEMETRY_ENDPOINT}
      KONG_CLUSTER_TELEMETRY_SERVER_NAME: ${KONG_CLUSTER_TELEMETRY_SERVER_NAME}
      KONG_CLUSTER_CERT: /kong-certs/tls.crt
      KONG_CLUSTER_CERT_KEY: /kong-certs/tls.key
      KONG_LUA_SSL_TRUSTED_CERTIFICATE: system
      KONG_KONNECT_MODE: "on"
      KONG_CLUSTER_DP_LABELS: type:docker-macOsArmOS
      KONG_ROUTER_FLAVOR: expressions
      KONG_LOG_LEVEL: debug
    networks:
      - hr-demo

    volumes:
      - ./kong-certs/:/kong-certs

  hr-mcp:
    build:
      context: ./hr-mcp-server
      dockerfile: Dockerfile
    container_name: hr-mcp-server
    ports:
      - "9000:9000"
    environment:
      PORT: "9000"
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:9000/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - hr-demo

  hr-agent:
    build:
      context: ./hr-agent
      dockerfile: Dockerfile
    container_name: hr-agent
    ports:
      - "8001:8001"
    environment:
      # LLM API settings - Kong AI Proxy handles everything
      LLM_API_URL: http://kong-gateway:8000/api/llm
      # MCP Server settings (always via Kong for token exchange demo)
      MCP_SERVER_URL: http://kong-gateway:8000/mcp
    depends_on:
      hr-mcp:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
    networks:
      - hr-demo

  streamlit-ui:
    build:
      context: ./streamlit-app
      dockerfile: Dockerfile
    container_name: streamlit-ui
    ports:
      - "8501:8501"
    extra_hosts:
      - "localhost:host-gateway"
    environment:
      # HR Agent endpoint (via Kong Gateway)
      AGENT_URL: http://kong-gateway:8000/api/agent
    depends_on:
      - hr-agent
    networks:
      - hr-demo

networks:
  hr-demo:
    name: hr-demo
    driver: bridge
